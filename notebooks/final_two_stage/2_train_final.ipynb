{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 – Training (Final)\n",
        "\n",
        "Dieses Notebook trainiert für eine gegebene `EXP_ID` das Zwei-Stufen-\n",
        "XGBoost-Modell (Signal + Richtung). Es liest die Config und den\n",
        "Trainingsdatensatz aus `data/processed/...` und erzeugt nur den\n",
        "Modell-Fit und einfache Metriken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0bb97ab7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erkannte Projektwurzel: /Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project\n",
            "Arbeitsverzeichnis gesetzt auf: /Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "cwd = Path.cwd()\n",
        "project_root = cwd\n",
        "while not (project_root / 'src').is_dir():\n",
        "    if project_root.parent == project_root:\n",
        "        raise RuntimeError(\"Projektwurzel mit 'src' nicht gefunden.\")\n",
        "    project_root = project_root.parent\n",
        "\n",
        "print('Erkannte Projektwurzel:', project_root)\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "os.chdir(project_root)\n",
        "print('Arbeitsverzeichnis gesetzt auf:', Path.cwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "51a56970",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bitte EXP_ID explizit setzen, passend zur Data-Prep.\n",
        "EXP_ID = 'hv5_h4_thr0p4pct_hit_5'  # z.B. 'v3_h4_thr0p3pct_relaxed'\n",
        "assert EXP_ID != 'CHANGE_ME', 'Bitte EXP_ID oben setzen.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "341e4c15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verwende Datensatz: data/processed/datasets/eurusd_news_training__hv5_h4_thr0p4pct_hit_5.csv\n",
            "(1163, 44)\n",
            "Feature-Spalten: 35\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from src.utils.io import DATA_PROCESSED\n",
        "from src.models.train_xgboost_two_stage import (\n",
        "    split_train_val_test,\n",
        "    build_signal_targets,\n",
        "    build_direction_targets,\n",
        "    train_xgb_binary,\n",
        "    get_feature_cols,\n",
        ")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Datensatz laden\n",
        "ds_path = DATA_PROCESSED / 'datasets' / f'eurusd_news_training__{EXP_ID}.csv'\n",
        "print('Verwende Datensatz:', ds_path)\n",
        "df = pd.read_csv(ds_path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n",
        "print(df.shape)\n",
        "feature_cols = get_feature_cols(df)\n",
        "print('Feature-Spalten:', len(feature_cols))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1767994a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 2020-04-14 -> 2024-04-09 n= 756\n",
            "val 2024-04-10 -> 2024-12-31 n= 189\n",
            "test 2025-01-02 -> 2025-11-05 n= 218\n"
          ]
        }
      ],
      "source": [
        "# Zeitliche Splits\n",
        "test_start = '2025-01-01'\n",
        "train_frac_pretest = 0.8\n",
        "\n",
        "splits = split_train_val_test(\n",
        "    df, pd.to_datetime(test_start), train_frac_within_pretest=train_frac_pretest\n",
        ")\n",
        "for name, split_df in splits.items():\n",
        "    print(name, split_df['date'].min().date(), '->', split_df['date'].max().date(), 'n=', len(split_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5d06f176",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Signal scale_pos_weight: 0.18309859154929578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ok] Signal-Modell trainiert.\n",
            "[ok] Richtungs-Modell trainiert.\n",
            "Richtungs-Schwelle (val-basiert): 0.4 F1_up(val): 0.7341772151898734\n",
            "[ok] Ergebnisse gespeichert unter:\n",
            "   JSON base : notebooks/results/two_stage__hv5_h4_thr0p4pct_hit_5.json\n",
            "   JSON final: notebooks/results/final_two_stage/two_stage_final__hv5_h4_thr0p4pct_hit_5.json\n",
            "   CSV final : notebooks/results/final_two_stage/two_stage_final__hv5_h4_thr0p4pct_hit_5_metrics.csv\n",
            "   Predictions: notebooks/results/final_two_stage/two_stage_final__hv5_h4_thr0p4pct_hit_5_predictions.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import json\n",
        "\n",
        "# Schwelle für das Signal-Modell (Stufe 1).\n",
        "# Höhere Werte -> höhere Precision, geringerer Recall.\n",
        "SIGNAL_THRESHOLD = 0.5\n",
        "\n",
        "# --- Signal-Modell trainieren ---\n",
        "y_train_signal = build_signal_targets(splits['train'])\n",
        "y_val_signal   = build_signal_targets(splits['val'])\n",
        "y_test_signal  = build_signal_targets(splits['test'])\n",
        "\n",
        "X_train_signal = splits['train'][feature_cols]\n",
        "X_val_signal   = splits['val'][feature_cols]\n",
        "X_test_signal  = splits['test'][feature_cols]\n",
        "\n",
        "# Class-Imbalance für das Signal-Modell explizit berücksichtigen\n",
        "n_pos_signal = int((y_train_signal == 1).sum())\n",
        "n_neg_signal = int((y_train_signal == 0).sum())\n",
        "scale_pos_weight_signal = n_neg_signal / max(n_pos_signal, 1)\n",
        "print('Signal scale_pos_weight:', scale_pos_weight_signal)\n",
        "\n",
        "model_signal = train_xgb_binary(\n",
        "    X_train_signal,\n",
        "    y_train_signal,\n",
        "    X_val_signal,\n",
        "    y_val_signal,\n",
        "    scale_pos_weight=scale_pos_weight_signal,\n",
        ")\n",
        "print('[ok] Signal-Modell trainiert.')\n",
        "\n",
        "# --- Richtungs-Modell trainieren ---\n",
        "X_train_dir, y_train_dir = build_direction_targets(splits['train'], feature_cols=feature_cols)\n",
        "X_val_dir,   y_val_dir   = build_direction_targets(splits['val'],   feature_cols=feature_cols)\n",
        "X_test_dir,  y_test_dir  = build_direction_targets(splits['test'],  feature_cols=feature_cols)\n",
        "\n",
        "model_dir = train_xgb_binary(\n",
        "    X_train_dir,\n",
        "    y_train_dir,\n",
        "    X_val_dir,\n",
        "    y_val_dir,\n",
        "    scale_pos_weight=1.0,\n",
        ")\n",
        "print('[ok] Richtungs-Modell trainiert.')\n",
        "\n",
        "# --- Metriken berechnen und speichern ---\n",
        "\n",
        "def binary_metrics_dict(y_true, y_prob, threshold, target_names):\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        target_names=target_names,\n",
        "        output_dict=True,\n",
        "        digits=3,\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred).tolist()\n",
        "    return {\n",
        "        'threshold': float(threshold),\n",
        "        'report': report,\n",
        "        'confusion_matrix': cm,\n",
        "    }\n",
        "\n",
        "# Wahrscheinlichkeiten\n",
        "p_train_signal = model_signal.predict_proba(X_train_signal)[:, 1]\n",
        "p_val_signal   = model_signal.predict_proba(X_val_signal)[:, 1]\n",
        "p_test_signal  = model_signal.predict_proba(X_test_signal)[:, 1]\n",
        "\n",
        "signal_metrics = {\n",
        "    'train': binary_metrics_dict(y_train_signal, p_train_signal, SIGNAL_THRESHOLD, ['neutral', 'move']),\n",
        "    'val':   binary_metrics_dict(y_val_signal,   p_val_signal,   SIGNAL_THRESHOLD, ['neutral', 'move']),\n",
        "    'test':  binary_metrics_dict(y_test_signal,  p_test_signal,  SIGNAL_THRESHOLD, ['neutral', 'move']),\n",
        "}\n",
        "\n",
        "p_train_dir = model_dir.predict_proba(X_train_dir)[:, 1]\n",
        "p_val_dir   = model_dir.predict_proba(X_val_dir)[:, 1]\n",
        "p_test_dir  = model_dir.predict_proba(X_test_dir)[:, 1]\n",
        "\n",
        "# Threshold für das Richtungs-Modell (down vs up) anhand des Val-Splits optimieren\n",
        "thr_grid = np.linspace(0.4, 0.6, 11)\n",
        "best_thr = 0.5\n",
        "best_f1_up = -1.0\n",
        "for thr in thr_grid:\n",
        "    y_val_pred = (p_val_dir >= thr).astype(int)\n",
        "    rep = classification_report(\n",
        "        y_val_dir,\n",
        "        y_val_pred,\n",
        "        target_names=['down', 'up'],\n",
        "        output_dict=True,\n",
        "        digits=3,\n",
        "    )\n",
        "    f1_up = rep['up']['f1-score']\n",
        "    if f1_up > best_f1_up:\n",
        "        best_f1_up = f1_up\n",
        "        best_thr = thr\n",
        "\n",
        "DIR_THRESHOLD = float(best_thr)\n",
        "print('Richtungs-Schwelle (val-basiert):', DIR_THRESHOLD, 'F1_up(val):', best_f1_up)\n",
        "\n",
        "direction_metrics = {\n",
        "    'train': binary_metrics_dict(y_train_dir, p_train_dir, DIR_THRESHOLD, ['down', 'up']),\n",
        "    'val':   binary_metrics_dict(y_val_dir,   p_val_dir,   DIR_THRESHOLD, ['down', 'up']),\n",
        "    'test':  binary_metrics_dict(y_test_dir,  p_test_dir,  DIR_THRESHOLD, ['down', 'up']),\n",
        "}\n",
        "\n",
        "# Kombinierte 3-Klassen-Auswertung auf Test\n",
        "X_test_all = splits['test'][feature_cols]\n",
        "signal_prob_test = model_signal.predict_proba(X_test_all)[:, 1]\n",
        "signal_pred_test = (signal_prob_test >= SIGNAL_THRESHOLD).astype(int)\n",
        "dir_prob_test = model_dir.predict_proba(X_test_all)[:, 1]\n",
        "dir_pred_test = (dir_prob_test >= DIR_THRESHOLD).astype(int)\n",
        "\n",
        "combined_pred = np.where(\n",
        "    signal_pred_test == 0,\n",
        "    'neutral',\n",
        "    np.where(dir_pred_test == 1, 'up', 'down'),\n",
        ")\n",
        "combined_true = splits['test']['label'].to_numpy()\n",
        "\n",
        "combined_report = classification_report(\n",
        "    combined_true,\n",
        "    combined_pred,\n",
        "    labels=['neutral', 'up', 'down'],\n",
        "    output_dict=True,\n",
        "    digits=3,\n",
        ")\n",
        "combined_cm = confusion_matrix(\n",
        "    combined_true,\n",
        "    combined_pred,\n",
        "    labels=['neutral', 'up', 'down'],\n",
        ").tolist()\n",
        "\n",
        "# Modell-Parameter + Feature-Importances\n",
        "signal_params = model_signal.get_xgb_params()\n",
        "direction_params = model_dir.get_xgb_params()\n",
        "signal_params['feature_importances_'] = model_signal.feature_importances_.tolist()\n",
        "direction_params['feature_importances_'] = model_dir.feature_importances_.tolist()\n",
        "\n",
        "# Config laden\n",
        "from src.utils.io import DATA_PROCESSED\n",
        "exp_meta_dir = DATA_PROCESSED / 'experiments'\n",
        "exp_config_path = exp_meta_dir / f'{EXP_ID}_config.json'\n",
        "with exp_config_path.open('r', encoding='utf-8') as f:\n",
        "    exp_config = json.load(f)\n",
        "\n",
        "config_block = {\n",
        "    'exp_id': exp_config.get('exp_id', EXP_ID),\n",
        "    'horizon_days': exp_config.get('label_params', {}).get('horizon_days'),\n",
        "    'up_threshold': exp_config.get('label_params', {}).get('up_threshold'),\n",
        "    'down_threshold': exp_config.get('label_params', {}).get('down_threshold'),\n",
        "    'strict_monotonic': exp_config.get('label_params', {}).get('strict_monotonic'),\n",
        "    'dataset_path': str(ds_path),\n",
        "    'feature_cols': feature_cols,\n",
        "    'test_start': test_start,\n",
        "    'train_frac_within_pretest': train_frac_pretest,\n",
        "    'signal_threshold': SIGNAL_THRESHOLD,\n",
        "    'direction_threshold': DIR_THRESHOLD,\n",
        "}\n",
        "\n",
        "results = {\n",
        "    'config': config_block,\n",
        "    'model_params': {\n",
        "        'signal': signal_params,\n",
        "        'direction': direction_params,\n",
        "    },\n",
        "    'signal': signal_metrics,\n",
        "    'direction': direction_metrics,\n",
        "    'combined_test': {\n",
        "        'report': combined_report,\n",
        "        'confusion_matrix': combined_cm,\n",
        "        'labels': ['neutral', 'up', 'down'],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Ergebnisse in Standard- und Final-Ordner schreiben\n",
        "base_results_dir = Path('notebooks') / 'results'\n",
        "final_results_dir = base_results_dir / 'final_two_stage'\n",
        "base_results_dir.mkdir(parents=True, exist_ok=True)\n",
        "final_results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "json_base = base_results_dir / f'two_stage__{EXP_ID}.json'\n",
        "json_final = final_results_dir / f'two_stage_final__{EXP_ID}.json'\n",
        "\n",
        "with json_base.open('w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "with json_final.open('w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# einfache Metrik-Tabelle (F1 der positiven Klasse)\n",
        "rows = []\n",
        "for model_key, model_name, pos_label in [\n",
        "    ('signal', 'signal', 'move'),\n",
        "    ('direction', 'direction', 'up'),\n",
        "]:\n",
        "    metrics = results[model_key]\n",
        "    for split, m in metrics.items():\n",
        "        rep = m['report']\n",
        "        cls = rep.get(pos_label, {})\n",
        "        rows.append({\n",
        "            'model': model_name,\n",
        "            'split': split,\n",
        "            'precision_pos': cls.get('precision'),\n",
        "            'recall_pos': cls.get('recall'),\n",
        "            'f1_pos': cls.get('f1-score'),\n",
        "        })\n",
        "\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "csv_final = final_results_dir / f'two_stage_final__{EXP_ID}_metrics.csv'\n",
        "metrics_df.to_csv(csv_final, index=False)\n",
        "\n",
        "# Test-Predictions als CSV für Fehlklassifikations-Analysen speichern\n",
        "test_dates = splits['test']['date'].to_numpy()\n",
        "test_labels = splits['test']['label'].to_numpy()\n",
        "\n",
        "pred_rows = []\n",
        "for dt, y_true, sig_p, sig_hat, dir_p, dir_hat, comb_hat in zip(\n",
        "    test_dates,\n",
        "    test_labels,\n",
        "    signal_prob_test,\n",
        "    signal_pred_test,\n",
        "    dir_prob_test,\n",
        "    dir_pred_test,\n",
        "    combined_pred,\n",
        "):\n",
        "    pred_rows.append({\n",
        "        'date': dt,\n",
        "        'label_true': y_true,\n",
        "        'signal_prob': float(sig_p),\n",
        "        'signal_pred': int(sig_hat),\n",
        "        'direction_prob_up': float(dir_p),\n",
        "        'direction_pred_up': int(dir_hat),\n",
        "        'combined_pred': comb_hat,\n",
        "    })\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows)\n",
        "pred_path = final_results_dir / f'two_stage_final__{EXP_ID}_predictions.csv'\n",
        "pred_df.to_csv(pred_path, index=False)\n",
        "\n",
        "print('[ok] Ergebnisse gespeichert unter:')\n",
        "print('   JSON base :', json_base)\n",
        "print('   JSON final:', json_final)\n",
        "print('   CSV final :', csv_final)\n",
        "print('   Predictions:', pred_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
