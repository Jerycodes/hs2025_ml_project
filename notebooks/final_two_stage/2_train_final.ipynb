{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 – Training (Final)\n",
        "\n",
        "Dieses Notebook trainiert für eine gegebene `EXP_ID` das Zwei-Stufen-\n",
        "XGBoost-Modell (Signal + Richtung). Es liest die Config und den\n",
        "Trainingsdatensatz aus `data/processed/...` und erzeugt nur den\n",
        "Modell-Fit und einfache Metriken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0bb97ab7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erkannte Projektwurzel: /Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project\n",
            "Arbeitsverzeichnis gesetzt auf: /Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "cwd = Path.cwd()\n",
        "project_root = cwd\n",
        "while not (project_root / 'src').is_dir():\n",
        "    if project_root.parent == project_root:\n",
        "        raise RuntimeError(\"Projektwurzel mit 'src' nicht gefunden.\")\n",
        "    project_root = project_root.parent\n",
        "\n",
        "print('Erkannte Projektwurzel:', project_root)\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "os.chdir(project_root)\n",
        "print('Arbeitsverzeichnis gesetzt auf:', Path.cwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "51a56970",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bitte EXP_ID explizit setzen, passend zur Data-Prep.\n",
        "EXP_ID = 'hv_flex_0_7_result'  # z.B. 'v3_h4_thr0p3pct_relaxed'\n",
        "assert EXP_ID != 'CHANGE_ME', 'Bitte EXP_ID oben setzen.'\n",
        "\n",
        "# True = mit News-Merge, False = nur Preise\n",
        "# Hinweis: Wenn eine Config-Datei aus Data-Prep existiert, überschreibt sie FEATURE_MODE automatisch.\n",
        "USE_NEWS = True\n",
        "FEATURE_MODE = 'news+price' if USE_NEWS else 'price_only'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "341e4c15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verwende Datensatz: data/processed/datasets/eurusd_news_training__hv_flex_0_7_result.csv\n",
            "(1168, 48)\n",
            "Feature-Spalten: 35\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from src.utils.io import DATA_PROCESSED\n",
        "from src.models.train_xgboost_two_stage import (\n",
        "    split_train_val_test,\n",
        "    build_signal_targets,\n",
        "    build_direction_targets,\n",
        "    train_xgb_binary,\n",
        "    get_feature_cols,\n",
        ")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Datensatz laden\n",
        "# Config aus Data-Prep lesen (falls vorhanden)\n",
        "exp_meta_dir = DATA_PROCESSED / 'experiments'\n",
        "exp_config_path = exp_meta_dir / f'{EXP_ID}_config.json'\n",
        "if exp_config_path.is_file():\n",
        "    with exp_config_path.open('r', encoding='utf-8') as f:\n",
        "        exp_config = json.load(f)\n",
        "    cfg_feature_mode = exp_config.get('feature_mode')\n",
        "    if cfg_feature_mode and cfg_feature_mode != FEATURE_MODE:\n",
        "        print(f'[info] FEATURE_MODE überschrieben durch Config: {FEATURE_MODE} -> {cfg_feature_mode}')\n",
        "        FEATURE_MODE = cfg_feature_mode\n",
        "\n",
        "ds_kind = 'news' if FEATURE_MODE == 'news+price' else 'price'\n",
        "ds_path = DATA_PROCESSED / 'datasets' / f'eurusd_{ds_kind}_training__{EXP_ID}.csv'\n",
        "print('Verwende Datensatz:', ds_path)\n",
        "if not ds_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Datensatz nicht gefunden: {ds_path}\\n\"\n",
        "        \"→ Bitte zuerst das Data-Prep-Notebook (1_data_prep_final.ipynb) mit genau derselben EXP_ID ausführen.\"\n",
        "    )\n",
        "df = pd.read_csv(ds_path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n",
        "print(df.shape)\n",
        "feature_cols = get_feature_cols(df)\n",
        "\n",
        "# Price-only: entferne News-Spalten aus feature_cols\n",
        "if FEATURE_MODE == 'price_only':\n",
        "    drop_prefixes = ['news_']\n",
        "    drop_exact = [\n",
        "        'article_count',\n",
        "        'avg_polarity',\n",
        "        'avg_neg',\n",
        "        'avg_neu',\n",
        "        'avg_pos',\n",
        "        'pos_share',\n",
        "        'neg_share',\n",
        "    ]\n",
        "    feature_cols = [\n",
        "        c\n",
        "        for c in feature_cols\n",
        "        if not any(c.startswith(p) for p in drop_prefixes) and c not in drop_exact\n",
        "    ]\n",
        "    print('[info] Price-only aktiv – News-Features entfernt.')\n",
        "\n",
        "print('Feature-Spalten:', len(feature_cols))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1767994a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 2020-04-14 -> 2023-11-28 n= 661\n",
            "val 2023-11-29 -> 2024-12-31 n= 284\n",
            "test 2025-01-02 -> 2025-11-12 n= 223\n"
          ]
        }
      ],
      "source": [
        "# Zeitliche Splits\n",
        "test_start = '2025-01-01'\n",
        "train_frac_pretest = 0.7\n",
        "\n",
        "splits = split_train_val_test(\n",
        "    df, pd.to_datetime(test_start), train_frac_within_pretest=train_frac_pretest\n",
        ")\n",
        "for name, split_df in splits.items():\n",
        "    print(name, split_df['date'].min().date(), '->', split_df['date'].max().date(), 'n=', len(split_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f44f2803",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_dir (503, 21)\n",
            "y_train_dir unique: (array([0, 1]), array([238, 265]))\n",
            "train signal counts: {0: 444, 1: 217}\n",
            "train direction counts (signal==1): {0.0: 122, 1.0: 95}\n"
          ]
        }
      ],
      "source": [
        "# Optional: Debug-Zelle (nur nötig, wenn du etwas prüfen willst).\n",
        "# Hinweis: Diese Zelle funktioniert erst, nachdem oben Datensatz+Splits geladen wurden.\n",
        "if 'splits' not in globals() or 'feature_cols' not in globals():\n",
        "    raise RuntimeError(\n",
        "        \"Bitte zuerst die Zellen oben ausführen (Datensatz laden + Splits + feature_cols).\"\n",
        "    )\n",
        "if 'X_train_dir' not in globals() or 'y_train_dir' not in globals():\n",
        "    X_train_dir, y_train_dir = build_direction_targets(splits['train'], feature_cols=feature_cols)\n",
        "\n",
        "print('X_train_dir', getattr(X_train_dir, 'shape', None))\n",
        "print('y_train_dir unique:', np.unique(y_train_dir, return_counts=True))\n",
        "print('train signal counts:', splits['train']['signal'].value_counts().to_dict())\n",
        "print('train direction counts (signal==1):', splits['train'].query('signal==1')['direction'].value_counts().to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5d06f176",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Signal scale_pos_weight: 2.046082949308756\n",
            "[ok] Signal-Modell trainiert.\n",
            "[debug] Signal train counts: {0: 444, 1: 217}\n",
            "[debug] Signal val counts  : {0: 236, 1: 48}\n",
            "[debug] Signal test counts : {0: 168, 1: 55}\n",
            "[debug] Direction train X/y: (217, 35) {0: 122, 1: 95}\n",
            "[debug] Direction val   X/y: (48, 35) {0: 27, 1: 21}\n",
            "[debug] Direction test  X/y: (55, 35) {0: 14, 1: 41}\n",
            "[ok] Richtungs-Modell trainiert.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
            "  warnings.warn(\n",
            "/Users/jeremynathan/Documents/GitHub/hs2025_ml_project/hs2025_ml_project/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Richtungs-Schwelle (val-basiert): 0.4 F1_up(val): 0.45714285714285713\n",
            "Richtungs-Schwellen (kostenbasiert, Val): 0.39999999999999997 0.575 P&L(val): 5.199999999999999\n",
            "Signal-Schwelle (kostenbasiert, Val): 0.5 P&L(val): 5.199999999999999\n",
            "[ok] Ergebnisse gespeichert unter:\n",
            "   JSON base : notebooks/results/two_stage__hv_flex_0_7_result.json\n",
            "   JSON final: notebooks/results/final_two_stage/two_stage_final__hv_flex_0_7_result.json\n",
            "   CSV final : notebooks/results/final_two_stage/two_stage_final__hv_flex_0_7_result_metrics.csv\n",
            "   Predictions: notebooks/results/final_two_stage/two_stage_final__hv_flex_0_7_result_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import json\n",
        "\n",
        "# Schwelle für das Signal-Modell (Stufe 1).\n",
        "# Höhere Werte -> höhere Precision, geringerer Recall.\n",
        "SIGNAL_THRESHOLD = 0.5\n",
        "\n",
        "# --- Signal-Modell trainieren ---\n",
        "y_train_signal = build_signal_targets(splits['train'])\n",
        "y_val_signal   = build_signal_targets(splits['val'])\n",
        "y_test_signal  = build_signal_targets(splits['test'])\n",
        "\n",
        "X_train_signal = splits['train'][feature_cols]\n",
        "X_val_signal   = splits['val'][feature_cols]\n",
        "X_test_signal  = splits['test'][feature_cols]\n",
        "\n",
        "# Class-Imbalance für das Signal-Modell explizit berücksichtigen\n",
        "n_pos_signal = int((y_train_signal == 1).sum())\n",
        "n_neg_signal = int((y_train_signal == 0).sum())\n",
        "scale_pos_weight_signal = n_neg_signal / max(n_pos_signal, 1)\n",
        "print('Signal scale_pos_weight:', scale_pos_weight_signal)\n",
        "\n",
        "model_signal = train_xgb_binary(\n",
        "    X_train_signal,\n",
        "    y_train_signal,\n",
        "    X_val_signal,\n",
        "    y_val_signal,\n",
        "    scale_pos_weight=scale_pos_weight_signal,\n",
        ")\n",
        "print('[ok] Signal-Modell trainiert.')\n",
        "\n",
        "# --- Richtungs-Modell trainieren ---\n",
        "X_train_dir, y_train_dir = build_direction_targets(splits['train'], feature_cols=feature_cols)\n",
        "X_val_dir,   y_val_dir   = build_direction_targets(splits['val'],   feature_cols=feature_cols)\n",
        "X_test_dir,  y_test_dir  = build_direction_targets(splits['test'],  feature_cols=feature_cols)\n",
        "\n",
        "def _count01(y):\n",
        "    if y is None or len(y) == 0:\n",
        "        return {}\n",
        "    u, c = np.unique(y, return_counts=True)\n",
        "    return {int(uu): int(cc) for uu, cc in zip(u, c)}\n",
        "\n",
        "print('[debug] Signal train counts:', _count01(y_train_signal))\n",
        "print('[debug] Signal val counts  :', _count01(y_val_signal))\n",
        "print('[debug] Signal test counts :', _count01(y_test_signal))\n",
        "print('[debug] Direction train X/y:', getattr(X_train_dir, 'shape', None), _count01(y_train_dir))\n",
        "print('[debug] Direction val   X/y:', getattr(X_val_dir, 'shape', None), _count01(y_val_dir))\n",
        "print('[debug] Direction test  X/y:', getattr(X_test_dir, 'shape', None), _count01(y_test_dir))\n",
        "if len(X_val_dir) == 0:\n",
        "    print(\"[warn] Val-Split hat 0 Bewegungstage (signal==1) → DIR_THRESHOLD kann nicht val-basiert optimiert werden.\")\n",
        "if len(X_test_dir) == 0:\n",
        "    print(\"[warn] Test-Split hat 0 Bewegungstage (signal==1) → Direction-Metriken sind leer.\")\n",
        "\n",
        "model_dir = train_xgb_binary(\n",
        "    X_train_dir,\n",
        "    y_train_dir,\n",
        "    X_val_dir,\n",
        "    y_val_dir,\n",
        "    scale_pos_weight=1.0,\n",
        ")\n",
        "print('[ok] Richtungs-Modell trainiert.')\n",
        "\n",
        "# --- Metriken berechnen und speichern ---\n",
        "\n",
        "def binary_metrics_dict(y_true, y_prob, threshold, target_names):\n",
        "    if y_true is None or len(y_true) == 0:\n",
        "        return {\n",
        "            'threshold': float(threshold),\n",
        "            'report': {},\n",
        "            'confusion_matrix': [],\n",
        "        }\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        target_names=target_names,\n",
        "        output_dict=True,\n",
        "        digits=3,\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred).tolist()\n",
        "    return {\n",
        "        'threshold': float(threshold),\n",
        "        'report': report,\n",
        "        'confusion_matrix': cm,\n",
        "    }\n",
        "\n",
        "def proba_pos(model, X):\n",
        "    \"\"\"P(positive Klasse) als 1D-Array; liefert [] wenn X leer ist.\"\"\"\n",
        "    if X is None or len(X) == 0:\n",
        "        return np.array([])\n",
        "    proba = model.predict_proba(X)\n",
        "    if getattr(proba, 'ndim', 0) != 2 or proba.shape[1] < 2:\n",
        "        raise ValueError(\n",
        "            f\"predict_proba lieferte unerwartete Form {getattr(proba, 'shape', None)}. \"\n",
        "            \"Das Modell ist evtl. degeneriert (z.B. Training hatte nur 1 Klasse oder leere Daten).\"\n",
        "        )\n",
        "    return proba[:, 1]\n",
        "\n",
        "# Wahrscheinlichkeiten\n",
        "p_train_signal = proba_pos(model_signal, X_train_signal)\n",
        "p_val_signal   = proba_pos(model_signal, X_val_signal)\n",
        "p_test_signal  = proba_pos(model_signal, X_test_signal)\n",
        "\n",
        "signal_metrics = {\n",
        "    'train': binary_metrics_dict(y_train_signal, p_train_signal, SIGNAL_THRESHOLD, ['neutral', 'move']),\n",
        "    'val':   binary_metrics_dict(y_val_signal,   p_val_signal,   SIGNAL_THRESHOLD, ['neutral', 'move']),\n",
        "    'test':  binary_metrics_dict(y_test_signal,  p_test_signal,  SIGNAL_THRESHOLD, ['neutral', 'move']),\n",
        "}\n",
        "\n",
        "p_train_dir = proba_pos(model_dir, X_train_dir)\n",
        "p_val_dir   = proba_pos(model_dir, X_val_dir)\n",
        "p_test_dir  = proba_pos(model_dir, X_test_dir)\n",
        "\n",
        "# Threshold für das Richtungs-Modell (down vs up) anhand des Val-Splits optimieren\n",
        "DIR_THRESHOLD = 0.5\n",
        "best_f1_up = None\n",
        "if y_val_dir is not None and len(y_val_dir) > 0 and len(np.unique(y_val_dir)) > 1:\n",
        "    thr_grid = np.linspace(0.4, 0.6, 11)\n",
        "    best_thr = 0.5\n",
        "    best_f1_up = -1.0\n",
        "    for thr in thr_grid:\n",
        "        y_val_pred = (p_val_dir >= thr).astype(int)\n",
        "        rep = classification_report(\n",
        "            y_val_dir,\n",
        "            y_val_pred,\n",
        "            target_names=['down', 'up'],\n",
        "            output_dict=True,\n",
        "            digits=3,\n",
        "            zero_division=0,\n",
        "        )\n",
        "        f1_up = rep.get('up', {}).get('f1-score', -1.0)\n",
        "        if f1_up > best_f1_up:\n",
        "            best_f1_up = f1_up\n",
        "            best_thr = thr\n",
        "    DIR_THRESHOLD = float(best_thr)\n",
        "    print('Richtungs-Schwelle (val-basiert):', DIR_THRESHOLD, 'F1_up(val):', best_f1_up)\n",
        "else:\n",
        "    print('[warn] Val-Split hat zu wenig Direction-Samples (0 oder nur 1 Klasse) → DIR_THRESHOLD=0.5')\n",
        "\n",
        "direction_metrics = {\n",
        "    'train': binary_metrics_dict(y_train_dir, p_train_dir, DIR_THRESHOLD, ['down', 'up']),\n",
        "    'val':   binary_metrics_dict(y_val_dir,   p_val_dir,   DIR_THRESHOLD, ['down', 'up']),\n",
        "    'test':  binary_metrics_dict(y_test_dir,  p_test_dir,  DIR_THRESHOLD, ['down', 'up']),\n",
        "}\n",
        "\n",
        "# --- Kostenbasierte Schwellen für das Richtungs-Modell bestimmen ---\n",
        "from src.utils.io import DATA_PROCESSED  # für Zugriff auf Experiment-Config\n",
        "exp_meta_dir = DATA_PROCESSED / 'experiments'\n",
        "exp_config_path = exp_meta_dir / f'{EXP_ID}_config.json'\n",
        "with exp_config_path.open('r', encoding='utf-8') as f:\n",
        "    _cfg = json.load(f)\n",
        "label_params = _cfg.get('label_params', {})\n",
        "up_thr_label = float(label_params.get('up_threshold', 0.0))\n",
        "down_thr_label = float(label_params.get('down_threshold', 0.0))\n",
        "max_adv_label = label_params.get('max_adverse_move_pct', 0.01) or 0.01\n",
        "\n",
        "# Einsatz-Größen für die Kostenfunktion (müssen zu Strategie A im Report passen)\n",
        "stake_up = 100.0\n",
        "stake_down = 100.0\n",
        "\n",
        "def cost_per_trade(true_label: str, pred_label: str) -> float:\n",
        "    \"\"\"Approx. Trade-Kosten in CHF für Strategie A.\n",
        "\n",
        "    Vereinfachte Annahme:\n",
        "    - Korrekte Trades verdienen ca. Schwelle * Einsatz.\n",
        "    - Falsche Trades bzw. Trades auf neutralen Tagen verlieren ca.\n",
        "      max_adverse_move_pct * Einsatz.\n",
        "    \"\"\"\n",
        "    true_label = str(true_label)\n",
        "    pred_label = str(pred_label)\n",
        "\n",
        "    if pred_label == 'neutral':\n",
        "        return 0.0\n",
        "    if true_label == 'neutral':\n",
        "        # konservativ: immer Stop-Loss\n",
        "        return -stake_up * max_adv_label if pred_label == 'up' else -stake_down * max_adv_label\n",
        "    if pred_label == 'up':\n",
        "        if true_label == 'up':\n",
        "            return stake_up * up_thr_label\n",
        "        else:  # true_label == 'down'\n",
        "            return -stake_up * max_adv_label\n",
        "    if pred_label == 'down':\n",
        "        if true_label == 'down':\n",
        "            return stake_down * (-down_thr_label)\n",
        "        else:  # true_label == 'up'\n",
        "            return -stake_down * max_adv_label\n",
        "    return 0.0\n",
        "\n",
        "labels_val = splits['val']['label'].to_numpy()\n",
        "signal_pred_val = (p_val_signal >= SIGNAL_THRESHOLD).astype(int)\n",
        "p_val_dir_all = proba_pos(model_dir, splits['val'][feature_cols])\n",
        "\n",
        "thr_candidates = np.linspace(0.3, 0.7, 17)\n",
        "best_pnl = -1e18\n",
        "best_thr_down = 0.4\n",
        "best_thr_up = 0.6\n",
        "\n",
        "for thr_down in thr_candidates:\n",
        "    for thr_up in thr_candidates:\n",
        "        if thr_down >= thr_up:\n",
        "            continue\n",
        "        pnl = 0.0\n",
        "        for prob, sig, true_lab in zip(p_val_dir_all, signal_pred_val, labels_val):\n",
        "            if sig == 0:\n",
        "                continue  # kein Trade, wenn Stufe 1 schon neutral ist\n",
        "            if prob >= thr_up:\n",
        "                pred_label = 'up'\n",
        "            elif prob <= thr_down:\n",
        "                pred_label = 'down'\n",
        "            else:\n",
        "                pred_label = 'neutral'\n",
        "            pnl += cost_per_trade(true_lab, pred_label)\n",
        "        if pnl > best_pnl:\n",
        "            best_pnl = pnl\n",
        "            best_thr_down = thr_down\n",
        "            best_thr_up = thr_up\n",
        "\n",
        "DIR_THR_DOWN = float(best_thr_down)\n",
        "DIR_THR_UP = float(best_thr_up)\n",
        "print('Richtungs-Schwellen (kostenbasiert, Val):', DIR_THR_DOWN, DIR_THR_UP, 'P&L(val):', best_pnl)\n",
        "\n",
        "# --- Kostenbasierte Schwelle für das Signal-Modell bestimmen ---\n",
        "thr_sig_candidates = np.linspace(0.3, 0.7, 17)\n",
        "best_sig_thr = SIGNAL_THRESHOLD\n",
        "best_sig_pnl = -1e18\n",
        "for thr_sig in thr_sig_candidates:\n",
        "    pnl = 0.0\n",
        "    for sig_prob, dir_prob, true_lab in zip(p_val_signal, p_val_dir_all, labels_val):\n",
        "        if sig_prob < thr_sig:\n",
        "            pred_label = 'neutral'\n",
        "        else:\n",
        "            if dir_prob >= DIR_THR_UP:\n",
        "                pred_label = 'up'\n",
        "            elif dir_prob <= DIR_THR_DOWN:\n",
        "                pred_label = 'down'\n",
        "            else:\n",
        "                pred_label = 'neutral'\n",
        "        pnl += cost_per_trade(true_lab, pred_label)\n",
        "    if pnl > best_sig_pnl:\n",
        "        best_sig_pnl = pnl\n",
        "        best_sig_thr = thr_sig\n",
        "\n",
        "SIG_THR_TRADE = float(best_sig_thr)\n",
        "print('Signal-Schwelle (kostenbasiert, Val):', SIG_THR_TRADE, 'P&L(val):', best_sig_pnl)\n",
        "\n",
        "# Kombinierte 3-Klassen-Auswertung auf Test\n",
        "X_test_all = splits['test'][feature_cols]\n",
        "signal_prob_test = proba_pos(model_signal, X_test_all)\n",
        "signal_pred_test = (signal_prob_test >= SIGNAL_THRESHOLD).astype(int)\n",
        "dir_prob_test = proba_pos(model_dir, X_test_all)\n",
        "\n",
        "combined_pred = np.full(len(signal_prob_test), 'neutral', dtype=object)\n",
        "mask_signal_trade = signal_prob_test >= SIG_THR_TRADE\n",
        "combined_pred[mask_signal_trade & (dir_prob_test >= DIR_THR_UP)] = 'up'\n",
        "combined_pred[mask_signal_trade & (dir_prob_test <= DIR_THR_DOWN)] = 'down'\n",
        "\n",
        "combined_true = splits['test']['label'].to_numpy()\n",
        "\n",
        "combined_report = classification_report(\n",
        "    combined_true,\n",
        "    combined_pred,\n",
        "    labels=['neutral', 'up', 'down'],\n",
        "    output_dict=True,\n",
        "    digits=3,\n",
        ")\n",
        "combined_cm = confusion_matrix(\n",
        "    combined_true,\n",
        "    combined_pred,\n",
        "    labels=['neutral', 'up', 'down'],\n",
        ").tolist()\n",
        "\n",
        "# Modell-Parameter + Feature-Importances\n",
        "signal_params = model_signal.get_xgb_params()\n",
        "direction_params = model_dir.get_xgb_params()\n",
        "signal_params['feature_importances_'] = model_signal.feature_importances_.tolist()\n",
        "direction_params['feature_importances_'] = model_dir.feature_importances_.tolist()\n",
        "\n",
        "# Config laden\n",
        "from src.utils.io import DATA_PROCESSED\n",
        "exp_meta_dir = DATA_PROCESSED / 'experiments'\n",
        "exp_config_path = exp_meta_dir / f'{EXP_ID}_config.json'\n",
        "with exp_config_path.open('r', encoding='utf-8') as f:\n",
        "    exp_config = json.load(f)\n",
        "\n",
        "feature_mode = FEATURE_MODE\n",
        "\n",
        "config_block = {\n",
        "    'exp_id': exp_config.get('exp_id', EXP_ID),\n",
        "    'price_source': exp_config.get('label_params', {}).get('price_source'),\n",
        "    'drop_weekends': exp_config.get('label_params', {}).get('drop_weekends'),\n",
        "    'horizon_days': exp_config.get('label_params', {}).get('horizon_days'),\n",
        "    'up_threshold': exp_config.get('label_params', {}).get('up_threshold'),\n",
        "    'down_threshold': exp_config.get('label_params', {}).get('down_threshold'),\n",
        "    'strict_monotonic': exp_config.get('label_params', {}).get('strict_monotonic'),\n",
        "    'max_adverse_move_pct': exp_config.get('label_params', {}).get('max_adverse_move_pct'),\n",
        "    'hit_within_horizon': exp_config.get('label_params', {}).get('hit_within_horizon'),\n",
        "    'first_hit_wins': exp_config.get('label_params', {}).get('first_hit_wins'),\n",
        "    'dataset_path': str(ds_path),\n",
        "    'feature_cols': feature_cols,\n",
        "    'test_start': test_start,\n",
        "    'train_frac_within_pretest': train_frac_pretest,\n",
        "    'signal_threshold': SIGNAL_THRESHOLD,\n",
        "    'signal_threshold_trade': SIG_THR_TRADE,\n",
        "    'direction_threshold': DIR_THRESHOLD,\n",
        "    'direction_threshold_down': DIR_THR_DOWN,\n",
        "    'direction_threshold_up': DIR_THR_UP,\n",
        "    'feature_mode': feature_mode,\n",
        "}\n",
        "\n",
        "results = {\n",
        "    'config': config_block,\n",
        "    'model_params': {\n",
        "        'signal': signal_params,\n",
        "        'direction': direction_params,\n",
        "    },\n",
        "    'signal': signal_metrics,\n",
        "    'direction': direction_metrics,\n",
        "    'combined_test': {\n",
        "        'report': combined_report,\n",
        "        'confusion_matrix': combined_cm,\n",
        "        'labels': ['neutral', 'up', 'down'],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Ergebnisse in Standard- und Final-Ordner schreiben\n",
        "base_results_dir = Path('notebooks') / 'results'\n",
        "final_results_dir = base_results_dir / 'final_two_stage'\n",
        "base_results_dir.mkdir(parents=True, exist_ok=True)\n",
        "final_results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "json_base = base_results_dir / f'two_stage__{EXP_ID}.json'\n",
        "json_final = final_results_dir / f'two_stage_final__{EXP_ID}.json'\n",
        "\n",
        "with json_base.open('w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "with json_final.open('w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# einfache Metrik-Tabelle (F1 der positiven Klasse)\n",
        "rows = []\n",
        "for model_key, model_name, pos_label in [\n",
        "    ('signal', 'signal', 'move'),\n",
        "    ('direction', 'direction', 'up'),\n",
        "]:\n",
        "    metrics = results[model_key]\n",
        "    for split, m in metrics.items():\n",
        "        rep = m['report']\n",
        "        cls = rep.get(pos_label, {})\n",
        "        rows.append({\n",
        "            'model': model_name,\n",
        "            'split': split,\n",
        "            'precision_pos': cls.get('precision'),\n",
        "            'recall_pos': cls.get('recall'),\n",
        "            'f1_pos': cls.get('f1-score'),\n",
        "        })\n",
        "\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "csv_final = final_results_dir / f'two_stage_final__{EXP_ID}_metrics.csv'\n",
        "metrics_df.to_csv(csv_final, index=False)\n",
        "\n",
        "# Test-Predictions als CSV für Fehlklassifikations-Analysen speichern\n",
        "test_dates = splits['test']['date'].to_numpy()\n",
        "test_labels = splits['test']['label'].to_numpy()\n",
        "\n",
        "pred_rows = []\n",
        "for dt, y_true, sig_p, sig_hat, dir_p, comb_hat in zip(\n",
        "    test_dates,\n",
        "    test_labels,\n",
        "    signal_prob_test,\n",
        "    signal_pred_test,\n",
        "    dir_prob_test,\n",
        "    combined_pred,\n",
        "):\n",
        "    pred_rows.append({\n",
        "        'date': dt,\n",
        "        'label_true': y_true,\n",
        "        'signal_prob': float(sig_p),\n",
        "        'signal_pred': int(sig_hat),\n",
        "        'direction_prob_up': float(dir_p),\n",
        "        'direction_pred_up': 1 if comb_hat == 'up' else (0 if comb_hat == 'down' else -1),\n",
        "        'combined_pred': comb_hat,\n",
        "    })\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows)\n",
        "pred_path = final_results_dir / f'two_stage_final__{EXP_ID}_predictions.csv'\n",
        "pred_df.to_csv(pred_path, index=False)\n",
        "\n",
        "print('[ok] Ergebnisse gespeichert unter:')\n",
        "print('   JSON base :', json_base)\n",
        "print('   JSON final:', json_final)\n",
        "print('   CSV final :', csv_final)\n",
        "print('   Predictions:', pred_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
